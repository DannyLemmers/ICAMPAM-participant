{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing the necessary libraries\n",
    "We will start with a few basic things that we have already written for you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scipy\n",
    "from scipy import io\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Loading in the data\n",
    "As the data from the sensors was retrieved and saved through matlab, the files are of .mat extension. As we don't consider loading in data part of the workshop, we have written this for you. If you do have questions about this part, feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Downloading the dataset\n",
    "folder_path = './ICAMPAM-participant-main/DATA'\n",
    "!gdown 1usBpQgNdMRgQgNKxgDVdUYEoER1cjss1 && unzip ICAMPAM-participant-main.zip\n",
    "# Defining load .mat files function:\n",
    "def load_data(file_path, sensor_names, key):\n",
    "    data = io.loadmat(file_path, squeeze_me = True)\n",
    "    sensor_data = {}\n",
    "    for sensor_name in sensor_names:\n",
    "        try:\n",
    "            sensor_data[sensor_name] = data[key][sensor_name]\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Error: Unknown sensor. Check if '{sensor_name}' is spelled correctly or exist\")\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Error: Unknown key. Check if '{key}' is spelled correctly or exist\")\n",
    "    return sensor_data\n",
    "\n",
    "# Define the sensor names\n",
    "sensor_names = ['hand_IMU','lowerarm_IMU', 'upperarm_IMU', 'shoulder_IMU', 'sternum_IMU']\n",
    "\n",
    "# Load each .m file\n",
    "Trials_left_acc = load_data(f'{folder_path}/trials_ACCbf_Left_p10.mat', sensor_names, 'trials_ACCbf_Left')\n",
    "Trials_right_acc = load_data(f'{folder_path}/trials_ACCbf_Right_p10.mat', sensor_names, 'trials_ACCbf_Right')\n",
    "Trials_R_Left_bf2seg= load_data (f'{folder_path}/Trials_R_Left_bf2seg_p10.mat', sensor_names, 'trials_R_Left_bf2seg')\n",
    "Trials_R_Right_bf2seg= load_data (f'{folder_path}/Trials_R_Right_bf2seg_p10.mat', sensor_names, 'trials_R_Right_bf2seg')\n",
    "Trial_L_raw_acc = load_data(f'{folder_path}/Trials_Left_acc_p10.mat', sensor_names, 'trials_acc')\n",
    "Trial_R_raw_acc = load_data(f'{folder_path}/Trials_Right_acc_p10.mat', sensor_names, 'trials_acc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Context: Intuition on accelerations\n",
    "Before we get into the analysis, let's briefly make sure everything is alright. We are doing a reach and return movement. Logically, in the direction of the reach movement (x-axis) we should expect a:\n",
    "1. Brief acceleration period to move our arm towards the object\n",
    "2. Deacceleration once we near the object. Slowing ourselves down to not overshoot the target\n",
    "3. Negative acceleration to move back towards the starting point.\n",
    "4. An acceleration to slow down the backwards motion and return to rest.\n",
    "\n",
    "If we have selected our trial correctly, this is what we expect to see. For now, we will only check the hand IMU, as the end effector gives the most intuitive results. Let's make some plots to confirm this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_acc_left_hand = Trial_L_raw_acc[\"hand_IMU\"].tolist()\n",
    "for i, accelerations in enumerate(raw_acc_left_hand):\n",
    "    plt.plot(accelerations[:,0])\n",
    "plt.title(\"Accelerations of the left hand\")\n",
    "plt.legend([\"Trial 1\" ,\"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 s]\")\n",
    "plt.show()\n",
    "\n",
    "raw_acc_right_hand = Trial_R_raw_acc[\"hand_IMU\"].tolist()\n",
    "for i, accelerations in enumerate(raw_acc_right_hand):\n",
    "    plt.plot(accelerations[:,0])\n",
    "plt.title(\"Accelerations of the right hand\")\n",
    "plt.legend([\"Trial 1\" , \"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 s]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can already see that the reaching movement on the right side is less well-defined than on the left side. Looking at this, it may be unsurprising for some of us to learn that the right side also happens to be the impaired arm. Let us keep that in mind for the next exercises and see if we can quantify that somehow. Although we have the general shape we excepted, we can see that the axes are mirrored: when we excepted a positive peak, we have a negative and vice-versa. This won't matter too much, except that our directions are reversed. Positive is backwards, negative is forwards.\n",
    "\n",
    "Let's make some observations that might give us some intuition about the quality features first.\n",
    "1. It appears that to capture the entire movement, we need more samples for the right arm than for the left arm (mind the x-axis). This means that the movements, probably, take longer to complete.\n",
    "2. The accelerations appear larger on the left arm. This could translate to the unimpaired arm having higher peak velocities.\n",
    "3. The accelerations on the left side appear to have less fluctuations, less noisy. Maybe the movement is also considered smoother.\n",
    "\n",
    "# Context: Correcting the offset\n",
    "Before we start, as the trial starts and ends with zero velocity, the mean accelerations should be zero. This is not the case right now, and we should fix that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def zero_mean(vector):\n",
    "    #This function takes a vector and adjusts the mean to zero\n",
    "    correction = np.mean(vector)\n",
    "    vector = vector - correction\n",
    "    return vector\n",
    "zero_mean_left_acc_hand = []\n",
    "for trial_num, trial_acc in enumerate(raw_acc_left_hand):\n",
    "    df_acc = pd.DataFrame(trial_acc, columns= [\"x\", \"y\", \"z\"])\n",
    "    df_acc[\"x\"] = zero_mean(df_acc[\"x\"])\n",
    "    df_acc[\"y\"] = zero_mean(df_acc[\"y\"])\n",
    "    df_acc[\"z\"] = zero_mean(df_acc[\"z\"])\n",
    "    zero_mean_left_acc_hand.append(df_acc)\n",
    "\n",
    "zero_mean_right_acc_hand = []\n",
    "for trial_num, trial_acc in enumerate(raw_acc_right_hand):\n",
    "    df_acc = pd.DataFrame(trial_acc, columns= [\"x\", \"y\", \"z\"])\n",
    "    df_acc[\"x\"] = zero_mean(df_acc[\"x\"])\n",
    "    df_acc[\"y\"] = zero_mean(df_acc[\"y\"])\n",
    "    df_acc[\"z\"] = zero_mean(df_acc[\"z\"])\n",
    "    zero_mean_right_acc_hand.append(df_acc)\n",
    "\n",
    "for i, accelerations in enumerate(zero_mean_left_acc_hand):\n",
    "    plt.plot(accelerations[\"x\"])\n",
    "plt.title(\"Accelerations of the left hand\")\n",
    "plt.legend([\"Trial 1\" , \"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 s]\")\n",
    "plt.show()\n",
    "\n",
    "for i, accelerations in enumerate(zero_mean_right_acc_hand):\n",
    "    plt.plot(accelerations[\"x\"])\n",
    "plt.title(\"Accelerations of the right hand\")\n",
    "plt.legend([\"Trial 1\" ,\"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 s]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise: Movement time\n",
    "Let us start with the first observation: The movement times. If we know the length of the trial data, we know how many samples it took to complete the movement. Then, if we also know the sample rate, we can tell how long the movement took by dividing the trial length by the sample frequency. Let's get started with the first coding exercise. We will have to write a simple function to calculate the movement time based on:\n",
    "1. The length of the trial, and\n",
    "2. The sample frequency\n",
    "\n",
    "We have added some fluff around so that you only need to fill in the actual function below in \"calc_movement_time\". Go ahead and define the variable \"total_time\" in the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_movement_time(acceleration, fs):\n",
    "    '''\n",
    "    Input: Acceleration of a single trial and sample frequency (fs)\n",
    "    Calculates the trial duration based on the length of trial and sample frequency\n",
    "    '''\n",
    "    #calculate the total time of the trial.\n",
    "    total_time = len(acceleration)/fs\n",
    "    return total_time\n",
    "\n",
    "sensor = \"hand_IMU\"\n",
    "fs = 50\n",
    "movement_times = []\n",
    "list_acc_data = [zero_mean_left_acc_hand, zero_mean_right_acc_hand]\n",
    "for acc_side in list_acc_data:\n",
    "    for trial_acc in acc_side:\n",
    "        #Calculate features\n",
    "        movement_time = calc_movement_time(trial_acc, fs)\n",
    "        #Append Features to list\n",
    "        movement_times.append(movement_time)\n",
    "\n",
    "trials = [1, 2, 3, 4, 5]\n",
    "plt.plot(trials, movement_times[0:5])\n",
    "plt.plot(trials, movement_times[5:])\n",
    "plt.title(\"Movement time of left and right arm\")\n",
    "plt.legend([\"Left\", \"Right\"])\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.xticks(trials)\n",
    "plt.ylabel(\"Movement time [s]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Based on this figure - and steering far, far away from statistics - it would appear that the movement times of left and right are indeed different. Feel free to create some other plots to observe the differences in movement time.\n",
    "\n",
    "# Context: Velocity\n",
    "\n",
    "Now let us dive into the velocities. To obtain the velocities we integrate the accelerations. there are some numerical errors that occur in this process. One source of these errors is that the accelerations is not a continuous function, but a discrete measurement, meaning we have a finite number of samples. In between these measurements we are missing information, and we need to make some assumptions about the values in between the samples to deal with this.\n",
    "\n",
    "Another source of errors is from the sensors itself. Since we have a relatively short measurement time, at least mathematically speaking, we cannot expect that this measurement noise is zero mean, even though the underlying process might be. As such, velocities measured from IMUs are not as accurate as velocities measured from a motion tracking based setup and can bias the velocities that we have.\n",
    "\n",
    "As not every participant has a background with numerical mathematics, we will supply the function to integrate accelerations to velocities. We are particularly interested in the first part of the movement, where the subject reaches towards the target. As such, we will also select only a part of the velocity. This has been done fairly crudely (Using the first zero crossing of the x-axis after a certain index), we are still researching better ways to automatically identify the end of a goal-directed movement. Furthermore, we have made all x-movements positive towards the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_velocities(acceleration):\n",
    "    '''\n",
    "    #Input: Acceleration and optional plot\n",
    "    #Calculates the velocities based on the accelerations with the assumption that v(0) = 0. Simpsons method was used to approximate the velocities.\n",
    "    Output: Velocities based on the accelerations with the assumption that v(0) = 0.\n",
    "    '''\n",
    "    #Simpsons integral has a higher order integration error than the standard forward euler method. This reduces integrations errors\n",
    "    vel_x = scipy.integrate.cumulative_simpson(acceleration[\"x\"], dx = 1/50)\n",
    "    #We are finding all indices where the signal crosses the x-axis\n",
    "    zero_crossings =  np.where(np.diff(np.sign(vel_x)))[0]\n",
    "\n",
    "    #Find the first time we cross the axis past 25 samples (to avoid early crosses from starting the movement). 25 was heuristically selected.\n",
    "    end_idx = zero_crossings[np.where(zero_crossings > 25)[0][0]]+1\n",
    "    vel_x = vel_x[0:end_idx]\n",
    "\n",
    "    #This makes forward movements towards the target positive.\n",
    "    vel_x = vel_x * np.sign(np.mean(vel_x))\n",
    "\n",
    "    vel_y = scipy.integrate.cumulative_simpson(acceleration[\"y\"], dx = 1/50)\n",
    "    vel_y = vel_y[0:end_idx]\n",
    "    vel_z = scipy.integrate.cumulative_simpson(acceleration[\"z\"], dx = 1/50)\n",
    "    vel_z = vel_z[0:end_idx]\n",
    "    df_vel = pd.DataFrame([vel_x, vel_y, vel_z], [\"x\", \"y\", \"z\"]).transpose()\n",
    "    df_vel[\"norm\"] = df_vel.apply(np.linalg.norm, axis = 1)\n",
    "\n",
    "    #Since we are cutting off the acceleration, we should also shorten the accelerations for other calculations.\n",
    "    acceleration = acceleration.iloc[0:end_idx, :]\n",
    "\n",
    "    return [df_vel, acceleration]\n",
    "list_acc_data_reduced = [[],[]]\n",
    "list_acc_data = [zero_mean_left_acc_hand, zero_mean_right_acc_hand]\n",
    "list_velocities = []\n",
    "for i,acc_side in enumerate(list_acc_data):\n",
    "    for j, trial_acc in enumerate(acc_side):\n",
    "        #Calculate features\n",
    "        [velocities, accelerations] = calc_velocities(trial_acc)\n",
    "        list_acc_data_reduced[i].append(accelerations)\n",
    "        list_velocities.append(velocities)\n",
    "    #plt.show()\n",
    "list_acc_data = list_acc_data_reduced\n",
    "\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        for k in range(0,5):  # Iterate over the first 4 dataframes or less if less than 4\n",
    "            axes[i, j].plot(list_velocities[k][axis[it]])\n",
    "        axes[i, j].set_title(f'Velocity of axis {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Sample')\n",
    "        axes[i, j].set_ylabel('Velocity [m/s]')\n",
    "        axes[i, j].grid(True)\n",
    "        plt.legend([\"Trial 1\" ,\"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "        it += 1  # Increment after plotting each axis\n",
    "fig.suptitle(\"Velocities of the left hand\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        for k in range(5,10):  # Iterate over the first 4 dataframes or less if less than 4\n",
    "            axes[i, j].plot(list_velocities[k][axis[it]])\n",
    "        axes[i, j].set_title(f'Velocity of axis {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Sample')\n",
    "        axes[i, j].set_ylabel('Velocity [m/s]')\n",
    "        axes[i, j].grid(True)\n",
    "        plt.legend([\"Trial 1\" ,\"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "        it += 1  # Increment after plotting each axis\n",
    "fig.suptitle(\"Velocities of the right hand\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "What things do you notice in the velocities?\n",
    "\n",
    "# Exercise: Peak Velocity\n",
    "Peak velocities really do not appear very different. But let's calculate them anyway. We already have the velocities. So, finding the peak should be relatively trivial. Let's get to it\n",
    "\n",
    "You can use the numpy (np) library using the np.max function to find the peak of velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_peak_vel(velocities):\n",
    "    '''\n",
    "    #Input: Velocity and axis\n",
    "    :param velocity: Velocity vector\n",
    "    :param axis: the axis of which to calculate the peak velocity\n",
    "    :return: peak absolute velocity of a given axis.\n",
    "    '''\n",
    "    peak_velocities = []\n",
    "    # Loop over the columns to find the peak velocities\n",
    "    for i,col in enumerate(['x', 'y', 'z', 'norm']):\n",
    "        velocity = velocities[col]\n",
    "        #Calculate the peak velocity and append to peak_velocities\n",
    "        peak_velocity = max(abs(velocity))\n",
    "        peak_velocities.append(peak_velocity)\n",
    "    return  peak_velocities\n",
    "\n",
    "\n",
    "peak_velocities  = []\n",
    "for velocities in list_velocities:\n",
    "    #Append Features to list\n",
    "    peak_velocity = calc_peak_vel(velocities)\n",
    "    peak_velocities.append(peak_velocity)\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "trials= [1, 2, 3, 4, 5]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axes[i,j].plot(trials, list(list(zip(*peak_velocities[0:5]))[it]))\n",
    "        axes[i,j].plot(trials, list(list(zip(*peak_velocities[5:]))[it]))\n",
    "        axes[i, j].set_title(f'Peak velocity of axis {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Trial')\n",
    "        axes[i, j].set_xticks(range(1,6))\n",
    "        axes[i, j].set_ylabel('Peak velocity [m/s]')\n",
    "        axes[i, j].grid(True)\n",
    "        axes[i, j].legend([\"Left\", \"Right\"])\n",
    "        it= it + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Interestingly enough, the peak velocities appear highest for the affected arm! There does seem to be quite a variability in the direction of the target. Clearly not enough data to make a strong conclusion about how well the peak velocities can define quality, but good to keep in mind nonetheless. Looking back into the initial velocity figure, that is not really surprising either. A potential explanation might be that the movement itself is not necessarily much slower here, but less efficient.\n",
    "\n",
    "# Exercise: Movement Smoothness - Log Dimensionless Jerk\n",
    "Let's get into an actual smoothness feature: The log dimensionless jerk. Very briefly, the log dimensionless jerk is the integral of squared absolute jerk (instantaneous rate of change of the accelerations) compensated by a factor (here we use $\\frac{\\text{(movement time)}^{3}}{(\\text{peak velocity)}^2}$). In formula form: $\\text{LDJ} = -ln( \\frac{t_{mov}^{3}}{v_{peak}^{2}} \\cdot \\int_{t0}^{t1}|a(t)dt|^2dt)$. For the integration we recommend using a standard scipy function (like scipy.integrate.simpson). $a(t)$ here is the acceleration. The minus at the end is to make sure that a less negative value also corresponds to a smoother movement.\n",
    "\n",
    "This is a complicated one. If you aren't an expert in how to convert mathematical functions into code, don't worry. We will talk you through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_log_dimensionless_jerk(peak_vel, acceleration, fs):\n",
    "    '''\n",
    "    #Input: movement_time, peak_vel, acceleration, axis\n",
    "    :param movement_time: Duration of the trial movement\n",
    "    :param peak_vel: The peak velocity during that specific trial\n",
    "    :param acceleration: Acceleration vector\n",
    "    :return: log dimensionless jerk of a given axis\n",
    "    '''\n",
    "    #Step 1 is to calculate the norm of the accelerations. We have been using this in velocities,\n",
    "    #but haven't made one for the accelerations. If you are lost, look in the code above\n",
    "    #where we calculated the velocities. You can use the function apply together with the np.linalgnorm.\n",
    "    acceleration = acceleration.copy()\n",
    "    acceleration.loc[:, \"norm\"] = acceleration.apply(np.linalg.norm, axis=1)\n",
    "\n",
    "    #We will need the time it takes between each sample later on, so let's calculate that now\n",
    "    dt = 1/fs\n",
    "\n",
    "    jerk_squared =[]\n",
    "    for axis in ['x', 'y', 'z', 'norm']:\n",
    "\n",
    "        #The jerk is defined as the derivative of the accelerations. An easy way to obtain this in discrete cases is to use the differences between samples. As the time step becomes smaller and smaller, this should converge to the true derivative.\n",
    "        # np.gradient can do this if we give a vector and the time steps (1/fs). Our advice is to make an array containing the jerk squared for each of the individual axes.\n",
    "\n",
    "        jerk = np.gradient(acceleration[axis], 1/50)\n",
    "\n",
    "        #The formula asks for the jerk squared. We do that in two steps. First the square, then append it to a list.\n",
    "        single_jerk_squared = jerk**2\n",
    "        jerk_squared.append(single_jerk_squared)\n",
    "\n",
    "    #We also need the movement time of this trial. Since we shortened the movement, lets re-evaluate it:\n",
    "    movement_time = len(acceleration)/fs\n",
    "\n",
    "    #Empty list to store the LDJ for each axis in.\n",
    "    log_dimensionless_jerk = []\n",
    "\n",
    "    # Loop over the columns to compute integrals and log_dimensionless_jerk\n",
    "    for i,col in enumerate(['x', 'y', 'z', 'norm']):\n",
    "        #We need to integrate the jerks_squared over the entire movement,\n",
    "        #since we have already sliced the data into trials, we can just integrate an entire movement.\n",
    "        # Scipy.integrate has some nice functions for this, like scipy.integrate.simpson().\n",
    "        # Don't forget to add the time steps in the integration and select the single axis.\n",
    "        integral_ajs = scipy.integrate.simpson(jerk_squared[i], dx = dt)\n",
    "\n",
    "        #Select the peak velocity for the right axis from peak_vel\n",
    "        peak_velocity_axis = peak_vel[i]\n",
    "\n",
    "        #Now we need to calculate the factor from the text above. Powers in python are ** or pow(value, exp).\n",
    "        scale = (movement_time**3) / peak_velocity_axis**2\n",
    "\n",
    "        #The next step is to multiply the integral of the jerk squared with the scale\n",
    "        dimensionless_jerk = scale*integral_ajs\n",
    "\n",
    "        #And now take the natural logarithm (np.log), multiply it by -1, and append it to the list.\n",
    "        log_dim_jerk = -np.log(dimensionless_jerk)\n",
    "\n",
    "        log_dimensionless_jerk.append(log_dim_jerk)\n",
    "    return  log_dimensionless_jerk\n",
    "\n",
    "list_LDJ = []\n",
    "i = 0\n",
    "\n",
    "for acc_side in list_acc_data:\n",
    "    for trial_acc in acc_side:\n",
    "        peak_velocity = peak_velocities[i]\n",
    "        LDJ = calc_log_dimensionless_jerk(peak_velocity, trial_acc, fs)\n",
    "        list_LDJ.append(LDJ)\n",
    "        i = i + 1\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "trials= [1, 2, 3, 4, 5]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_LDJ[0:5]))[it]))\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_LDJ[5:]))[it]))\n",
    "        axes[i, j].set_title(f'Log dimensionless jerk of {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Trial')\n",
    "        axes[i, j].set_xticks(range(1,6))\n",
    "        axes[i, j].set_ylabel('Log dimensionless jerk [1]')\n",
    "        axes[i, j].grid(True)\n",
    "        axes[i, j].legend([\"Left\", \"Right\"])\n",
    "        it= it + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " In log dimensionless jerk, a less negative means a smoother movement. Here we see that the movements from the left side are indeed smoother than the right - the affected - side. Contrary to the peak velocities, these differences appear consistent in the participant. Remember, the jerk is directly related to the changes in the acceleration, if there is a lot of noise in the measurement, the changes due to the movement become difficult to untangle from the noise. This is important to keep in mind in your own applications.\n",
    "\n",
    "# Context: Movement smoothness - SPectral ARClength\n",
    "\n",
    "Perhaps SPARC will do better. SPARC or the SPectral ARClength is calculated by obtaining the fourier transform of the velocity profile within a specific frequency range. Mathematically: $-\\int^{\\omega_{c}}_0\\sqrt{(\\frac{1}{\\omega_c})^2+(\\frac{d\\hat{V}(\\omega)}{d \\omega})^2}d\\omega$ with $\\hat{V}(\\omega) = \\frac{V(\\omega)}{V(0)}$ and $\\omega_c \\triangleq min\\{w_c^{max}, min\\{\\omega, \\hat{V}(r) < \\bar{V} \\text{ } \\forall \\text{ } r>\\omega\\}\\}$. The first term is used to normalize the arc length with respect to the cut-off frequency $\\omega_{c}$\n",
    "\n",
    "Again, don't get scared by the math. This is a more complicated metric to calculate, so we have taken the liberty to do it for you. We followed the MATLAB implementation of Balasubramanian, but with some minor changes to make it work in Python. If you have any questions about the implementation, feel free to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_SPARC(velocity, fs, parameters=[0.05, 20, 4]):\n",
    "    '''\n",
    "    #Input: Velocity, fs and axis optional parameters\n",
    "    :param velocity: Velocity vector\n",
    "    :param fs: Sample frequency\n",
    "    :param axis: Axis of which to calculate the SPARC\n",
    "    :param parameters: Parameters of the SPARC, default [0.05, 20, 4] as recommended by Balasubramanian. The first number is the amplitude threshold. The second number is the maximum frequency cut-off and the third contains the zero padding.\n",
    "    :return: The SPARC of a given velocity profile on a specified axis\n",
    "    '''\n",
    "    #We make an empty list to hold the SPARC for each of the axes\n",
    "    SPARC = []\n",
    "    for i,axis in enumerate(['x', 'y', 'z', 'norm']):\n",
    "        #First we need to determine the number of points for the Fast Fourier Transform\n",
    "        N = len(velocity[axis])\n",
    "        #Add parameter[2] for the zero padding. Lower than 4 is not recommended. Numpy uses the Cooley-Turkey algorithm to calculate the FFT. This algorithm is optimal for data with a length equal to a power of 2. For computational efficiency we can include this, but it is not necessary.\n",
    "        Nfft = int(2**np.ceil(np.log2(N)+parameters[2]))\n",
    "        #Calculate the fast fourier transform of the velocity profile\n",
    "        vel_spect = np.abs(np.fft.fft(velocity[axis], Nfft))\n",
    "\n",
    "        #Create a frequency vector\n",
    "        dt = 1/fs\n",
    "        freq = np.arange(0, ((1/dt)*(Nfft-1)/Nfft) + (1/dt)*(1/Nfft), (1/dt)*(1/Nfft))\n",
    "\n",
    "        #Normalize the spectrum\n",
    "        vel_spect = vel_spect/np.max(vel_spect)\n",
    "\n",
    "        #Find the index that is always above the amplitude threshold and within the cut-off frequency\n",
    "        inxFc = np.where((freq <= parameters[1]) & (vel_spect >= parameters[0]))[0][-1]\n",
    "\n",
    "        #Select the relevant spectrum\n",
    "        vel_spect = vel_spect[0:inxFc]\n",
    "\n",
    "        #Calculate the arc-length\n",
    "        dArcLength = np.sqrt((1/inxFc))**2 + np.diff(vel_spect)**2\n",
    "        #and sum the arc-lengths for the smoothness and multiply by -1\n",
    "        SPARC.append(-1*np.sum(dArcLength))\n",
    "    return SPARC\n",
    "\n",
    "fs = 50\n",
    "i = 0\n",
    "#list_acc_data = [zero_mean_left_acc_hand, zero_mean_right_acc_hand]\n",
    "list_SPARC = []\n",
    "for acc_side in list_acc_data:\n",
    "    for trial_acc in acc_side:\n",
    "        SPARC = calc_SPARC(list_velocities[i], fs)\n",
    "        #Append Features to list\n",
    "        list_SPARC.append(SPARC)\n",
    "        i = i+1\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "trials= [1, 2, 3, 4, 5]\n",
    "\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_SPARC[0:5]))[it]))\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_SPARC[5:]))[it]))\n",
    "        axes[i, j].set_title(f'SPARC of {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Trial')\n",
    "        axes[i, j].set_xticks(range(1,6))\n",
    "        axes[i, j].set_ylabel('SPARC [1]')\n",
    "        axes[i, j].grid(True)\n",
    "        axes[i, j].legend([\"Left\", \"Right\"])\n",
    "        it= it + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here, a less negative SPARC indicate a smoother movement. Here too, we see that the movements of the left hand is smoother than the right hand. Both SPARC and LDJ here are able to indicate a difference in movement smoothness in all axes.\n",
    "\n",
    "# Joint angles\n",
    "\n",
    "To calculate the elbow flexion angle, we need to find the angle between two adjacent limbs, so in this case, lower arm and upper arm.\n",
    "The angle is calculated by first projecting the longitudinal axis of the most distal segment (in that case, lower arm) in the XZ plane (sagital plane) of the upper arm.\n",
    "Taking into account that the norm vector is in the mediolateral direction.\n",
    "\n",
    "The V_projected is obtained by this formula :\n",
    "$\\vec{v^p} = \\vec{v} - \\frac{\\vec{v}\\cdot\\vec{n}}{||\\vec{n}^2||} \\vec{n}$\n",
    "\n",
    "\n",
    "Thereafter the angle Theta is calculated with this formula:\n",
    "$\\Theta = \\text{atan2}(\\frac{||\\vec{v_1} \\times \\vec{v_2}||}{\\vec{v_1}\\cdot\\vec{v_2}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_flexion_angle(Trials_R_bf2seg, side):\n",
    "    upper = Trials_R_bf2seg['upperarm_IMU'].tolist()\n",
    "    lower = Trials_R_bf2seg['lowerarm_IMU'].tolist()\n",
    "\n",
    "    elbow_flexion_angle_alltrial = []\n",
    "\n",
    "    for j in range(len(upper)):\n",
    "        elbow_flexion_angle_singletrial = []\n",
    "\n",
    "        for i in range(min(len(upper[j]), len(lower[j]))):\n",
    "            x_wrist_projected = lower[j][i,:,0] - (np.dot(lower[j][i,:,0], upper[j][i,:,2]) / np.linalg.norm(upper[j][i,:,2])**2) * upper[j][i,:,2]\n",
    "            Y = np.linalg.norm(np.cross(x_wrist_projected, upper[j][i,:,1]))\n",
    "            X = np.dot(x_wrist_projected, upper[j][i,:,1])\n",
    "            elbow_flexion_angle = np.arctan2(Y, X)\n",
    "            elbow_flexion_angle = (elbow_flexion_angle / np.pi * 180)#*-1+240\n",
    "\n",
    "            if side == \"left\":\n",
    "                elbow_flexion_angle= elbow_flexion_angle# - 60\n",
    "\n",
    "            elbow_flexion_angle_singletrial.append(elbow_flexion_angle)\n",
    "\n",
    "        elbow_flexion_angle_alltrial.append(elbow_flexion_angle_singletrial)\n",
    "\n",
    "    return elbow_flexion_angle_alltrial\n",
    "\n",
    "# Calculate elbow flexion angles for left and right sides\n",
    "elbow_flexion_angle_left = elbow_flexion_angle(Trials_R_Left_bf2seg, \"left\")\n",
    "elbow_flexion_angle_right = elbow_flexion_angle(Trials_R_Right_bf2seg, \"right\")\n",
    "\n",
    "\n",
    "# Plot results for left and right sides\n",
    "num_trials = 5\n",
    "fig, axes = plt.subplots(num_trials, 2, figsize=(16, num_trials*4))\n",
    "\n",
    "for i in range(num_trials):\n",
    "    axes[i, 0].plot(elbow_flexion_angle_left[i])\n",
    "    axes[i, 0].set_title('Left Trial {}'.format(i+1))\n",
    "    axes[i, 0].set_xlabel('Time Point')\n",
    "    axes[i, 0].set_ylabel('Elbow Flexion Angle (degrees)')\n",
    "    axes[i, 0].grid(True)\n",
    "\n",
    "    axes[i, 1].plot(elbow_flexion_angle_right[i])\n",
    "    axes[i, 1].set_title('Right Trial {}'.format(i+1))\n",
    "    axes[i, 1].set_xlabel('Time Point')\n",
    "    axes[i, 1].set_ylabel('Elbow Flexion Angle (degrees)')\n",
    "    axes[i, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZic74y864btEBRqSPtj9+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
