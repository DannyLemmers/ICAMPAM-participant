{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up the Google Colab\n",
    "The easiest way to run this is through Google Colab. However, to get it the colab to access the data we will need to give access to the data. The easiest way to is to download the GitHub page and upload it to Google Drive. If you keep the same name as the repo (ICAMPAM-participant-main), you should be able to just run it. Otherwise, change the 'folder_name' variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "folder_path = './ICAMPAM-participant-main/DATA'\n",
    "!gdown 1usBpQgNdMRgQgNKxgDVdUYEoER1cjss1 && unzip ICAMPAM-participant-main.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing the necessary libraries\n",
    "We will start with a few basic things that we have already written for you. We have imported a few libraries, but if you feel like another library is more fitting,\n",
    "or you are more familiar with another library, feel free to use that one instead.\n",
    "\n",
    "If you are using your own python install, make sure scipy's version is not older than 1.12. We are using a scipy function (cumulative simpson) for the integration of the accelerations, and this was introduced since v.1.12."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scipy\n",
    "from scipy import io\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Loading in the data\n",
    "As the data from the sensors was retrieved and saved through matlab, the files are of .mat extension. The io library has a function for reading these files, but it doesn't load the data in a nice and ready-to-use format. For that, we will need to select the right sensors (sensor_names) and name of the original matlab variable (key). As we don't consider loading in data part of the workshop, we have written this for you. If you do have questions about this part, feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining load .mat files function:\n",
    "\n",
    "def load_data(file_path, sensor_names, key):\n",
    "    data = io.loadmat(file_path, squeeze_me = True)\n",
    "    sensor_data = {}\n",
    "    for sensor_name in sensor_names:\n",
    "        try:\n",
    "            sensor_data[sensor_name] = data[key][sensor_name]\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Error: Unknown sensor. Check if '{sensor_name}' is spelled correctly or exist\")\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Error: Unknown key. Check if '{key}' is spelled correctly or exist\")\n",
    "    return sensor_data\n",
    "\n",
    "# Define the sensor names\n",
    "sensor_names = ['hand_IMU','lowerarm_IMU', 'upperarm_IMU', 'shoulder_IMU', 'sternum_IMU']\n",
    "\n",
    "# Load each .m file\n",
    "Trials_left_acc = load_data(f'{folder_path}/trials_ACCbf_Left_p10.mat', sensor_names, 'trials_ACCbf_Left')\n",
    "Trials_right_acc = load_data(f'{folder_path}/trials_ACCbf_Right_p10.mat', sensor_names, 'trials_ACCbf_Right')\n",
    "Trials_R_Left_bf2seg= load_data (f'{folder_path}/Trials_R_Left_bf2seg_p10.mat', sensor_names, 'trials_R_Left_bf2seg')\n",
    "Trials_R_Right_bf2seg= load_data (f'{folder_path}/Trials_R_Right_bf2seg_p10.mat', sensor_names, 'trials_R_Right_bf2seg')\n",
    "Trial_L_raw_acc = load_data(f'{folder_path}/Trials_Left_acc_p10.mat', sensor_names, 'trials_acc')\n",
    "Trial_R_raw_acc = load_data(f'{folder_path}/Trials_Right_acc_p10.mat', sensor_names, 'trials_acc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Intuition on accelerations\n",
    "Before we get into the analysis, let's briefly make sure everything is alright. We are doing a reach and return movement. Logically, in the direction of the reach movement (x-axis) we should expect a:\n",
    "1. Brief acceleration period to move our arm towards the object\n",
    "2. Deacceleration once we near the object. Slowing ourselves down to not overshoot the target\n",
    "3. Negative acceleration to move back towards the starting point.\n",
    "4. An acceleration to slow down the backwards motion and return to rest.\n",
    "\n",
    "If we have selected our trial correctly, this is what we expect to see. For now, we will only check the hand IMU, as the end effector gives the most intuitive results. Let's make some plots to confirm this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_acc_left_hand = Trial_L_raw_acc[\"hand_IMU\"].tolist()\n",
    "for i, accelerations in enumerate(raw_acc_left_hand):\n",
    "    plt.plot(accelerations[:,0])\n",
    "plt.title(\"Accelerations of the left hand\")\n",
    "plt.legend([\"Trial 1\" ,\"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 ms]\")\n",
    "plt.show()\n",
    "\n",
    "raw_acc_right_hand = Trial_R_raw_acc[\"hand_IMU\"].tolist()\n",
    "for i, accelerations in enumerate(raw_acc_right_hand):\n",
    "    plt.plot(accelerations[:,0])\n",
    "plt.title(\"Accelerations of the right hand\")\n",
    "plt.legend([\"Trial 1\" , \"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 ms]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Correcting the offset\n",
    "We can already see that the reaching movement on the right side is less well-defined than on the left side. Looking at this, it may be unsurprising for some of us to learn that the right side also happens to be the impaired arm. Let us keep that in mind for the next exercises and see if we can quantify that somehow. Although we have the general shape we excepted, we can see that the axes are mirrored: when we excepted a positive peak, we have a negative and vice-versa. This won't matter too much, except that our directions are reversed. Positive is backwards, negative is forwards.\n",
    "\n",
    "Let's make some observations that might give us some intuition about the quality features first.\n",
    "1. It appears that to capture the entire movement, we need more samples for the right arm than for the left arm (mind the x-axis). This means that the movements, probably, take longer to complete.\n",
    "2. The accelerations appear larger on the left arm. This could translate to the unimpaired arm having higher peak velocities.\n",
    "3. The accelerations on the left side appear to have less fluctuations, less noisy. Maybe the movement is also considered smoother.\n",
    "\n",
    "Before we start, as the trial starts and ends with zero velocity, the mean accelerations should be zero. This is not the case right now, and we should fix that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def zero_mean(vector):\n",
    "    #This function takes a vector and adjusts the mean to zero\n",
    "    correction = np.mean(vector)\n",
    "    vector = vector - correction\n",
    "    return vector\n",
    "zero_mean_left_acc_hand = []\n",
    "for trial_num, trial_acc in enumerate(raw_acc_left_hand):\n",
    "    df_acc = pd.DataFrame(trial_acc, columns= [\"x\", \"y\", \"z\"])\n",
    "    df_acc[\"x\"] = zero_mean(df_acc[\"x\"])\n",
    "    df_acc[\"y\"] = zero_mean(df_acc[\"y\"])\n",
    "    df_acc[\"z\"] = zero_mean(df_acc[\"z\"])\n",
    "    zero_mean_left_acc_hand.append(df_acc)\n",
    "\n",
    "zero_mean_right_acc_hand = []\n",
    "for trial_num, trial_acc in enumerate(raw_acc_right_hand):\n",
    "    df_acc = pd.DataFrame(trial_acc, columns= [\"x\", \"y\", \"z\"])\n",
    "    df_acc[\"x\"] = zero_mean(df_acc[\"x\"])\n",
    "    df_acc[\"y\"] = zero_mean(df_acc[\"y\"])\n",
    "    df_acc[\"z\"] = zero_mean(df_acc[\"z\"])\n",
    "    zero_mean_right_acc_hand.append(df_acc)\n",
    "\n",
    "for i, accelerations in enumerate(zero_mean_left_acc_hand):\n",
    "    plt.plot(accelerations[\"x\"])\n",
    "plt.title(\"Accelerations of the left hand\")\n",
    "plt.legend([\"Trial 1\" , \"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 ms]\")\n",
    "plt.show()\n",
    "\n",
    "for i, accelerations in enumerate(zero_mean_right_acc_hand):\n",
    "    plt.plot(accelerations[\"x\"])\n",
    "plt.title(\"Accelerations of the right hand\")\n",
    "plt.legend([\"Trial 1\" ,\"Trial 2\", \"Trial 3\", \"Trial 4\", \"Trial 5\"])\n",
    "plt.ylabel(\"Acceleration [m/s^2]\")\n",
    "plt.xlabel(\"Sample [1/50 ms]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Movement time\n",
    "Let us start with the first observation: The movement times. If know the length of the trial data, we know how many samples it took to complete the movement. Then, if we also know the sample rate, we can tell how long the movement took by dividing the trial length by the sample frequency. Let's get started with the first coding exercise. We will have to write a simple function to calculate the movement time based on:\n",
    "1. The length of the trial, and\n",
    "2. The sample frequency\n",
    "\n",
    "We have added some fluff around so that you only need to fill in the actual function below in \"calc_movement_time\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_movement_time(acceleration, fs):\n",
    "    '''\n",
    "    Input: Acceleration of a single trial and sample frequency (fs)\n",
    "    Calculates the trial duration based on the length of trial and sample frequency\n",
    "    '''\n",
    "    #calculate the total time of the trial.\n",
    "\n",
    "    return total_time\n",
    "\n",
    "sensor = \"hand_IMU\"\n",
    "fs = 50\n",
    "movement_times = []\n",
    "list_acc_data = [zero_mean_left_acc_hand, zero_mean_right_acc_hand]\n",
    "for acc_side in list_acc_data:\n",
    "    for trial_acc in acc_side:\n",
    "        #Calculate features\n",
    "        movement_time = calc_movement_time(trial_acc, fs)\n",
    "        #Append Features to list\n",
    "        movement_times.append(movement_time)\n",
    "\n",
    "trials = [1, 2, 3, 4, 5]\n",
    "plt.plot(trials, movement_times[0:5])\n",
    "plt.plot(trials, movement_times[5:])\n",
    "plt.title(\"Movement time of left and right arm\")\n",
    "plt.legend([\"Left\", \"Right\"])\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.xticks(trials)\n",
    "plt.ylabel(\"Movement time [s]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Based on this figure - and steering far, far away from statistics - it would appear that the movement times of left and right are indeed different. Feel free to create some other plots to observe the differences in movement time.\n",
    "\n",
    "# Velocity\n",
    "\n",
    "Now let us dive into the velocities. First and foremost, a disclaimer. Since we are using accelerations and integrating these to obtain velocities, there are some errors that occur and propagate. One source of these errors is that the accelerations is not a continuous function, but a discrete measurement, meaning we have a finite number of samples. In between these measurements we are missing information, and we need to make some assumptions to deal with this.\n",
    "\n",
    "Another source of errors is from the sensors itself. Since we have a relatively short measurement time, at least mathematically speaking, we cannot expect that this measurement noise is zero mean, even though the underlying process might be. As such, velocities measured from IMUs are not as accurate as velocities measured from a motion tracking based set-up and can bias the velocities that we have.\n",
    "\n",
    "As not every participant has a background with numerical mathematics, we will supply the function to integrate accelerations to velocities. We are particularly interested in the first part of the movement, where the subject reaches towards the target. As such, we will also select only a part of the velocity. This has been done fairly crudely (Using the first zero crossing of the x-axis after a certain index), we are still researching better ways to automatically identify the end of a goal-directed movement. Furthermore, we have made all x-movements positive towards the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_velocities(acceleration):\n",
    "    '''\n",
    "    #Input: Acceleration and optional plot\n",
    "    #Calculates the velocities based on the accelerations with the assumption that v(0) = 0. Simpsons method was used to approximate the velocities. Optional argument plot to specify whether to plot the velocities or not.\n",
    "    Output: Velocities based on the accelerations with the assumption that v(0) = 0.\n",
    "    '''\n",
    "    #Simpsons integral has a higher order integration error than the standard forward euler method. This basically attempts to reduce the errors in the velocities\n",
    "    vel_x = scipy.integrate.cumulative_simpson(acceleration[\"x\"], dx = 1/50)\n",
    "    #We are finding all indices where the signal crosses the x-axis\n",
    "    zero_crossings =  np.where(np.diff(np.sign(vel_x)))[0]\n",
    "\n",
    "    #Find the first time we cross the axis past 25 samples (to avoid early crosses from starting the movement). 25 was heuristically selected.\n",
    "    end_idx = zero_crossings[np.where(zero_crossings > 25)[0][0]]+1\n",
    "    vel_x = vel_x[0:end_idx]\n",
    "\n",
    "    #This makes forward movements towards the target positive.\n",
    "    vel_x = vel_x * np.sign(np.mean(vel_x))\n",
    "\n",
    "    vel_y = scipy.integrate.cumulative_simpson(acceleration[\"y\"], dx = 1/50)\n",
    "    vel_y = vel_y[0:end_idx]\n",
    "    vel_z = scipy.integrate.cumulative_simpson(acceleration[\"z\"], dx = 1/50)\n",
    "    vel_z = vel_z[0:end_idx]\n",
    "    df_vel = pd.DataFrame([vel_x, vel_y, vel_z], [\"x\", \"y\", \"z\"]).transpose()\n",
    "    df_vel[\"norm\"] = df_vel.apply(np.linalg.norm, axis = 1)\n",
    "\n",
    "    #Since we are cutting off the acceleration, we should also shorten the accelerations for other calculations.\n",
    "    acceleration = acceleration.iloc[0:end_idx, :]\n",
    "\n",
    "    return [df_vel, acceleration]\n",
    "list_acc_data_reduced = [[],[]]\n",
    "list_acc_data = [zero_mean_left_acc_hand, zero_mean_right_acc_hand]\n",
    "list_velocities = []\n",
    "for i,acc_side in enumerate(list_acc_data):\n",
    "    for j, trial_acc in enumerate(acc_side):\n",
    "        #Calculate features\n",
    "        [velocities, accelerations] = calc_velocities(trial_acc)\n",
    "        list_acc_data_reduced[i].append(accelerations)\n",
    "        list_velocities.append(velocities)\n",
    "    #plt.show()\n",
    "list_acc_data = list_acc_data_reduced\n",
    "\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        for k in range(0,5):  # Iterate over the first 4 dataframes or less if less than 4\n",
    "            axes[i, j].plot(list_velocities[k][axis[it]])\n",
    "        axes[i, j].set_title(f'Velocity of axis {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Sample')\n",
    "        axes[i, j].set_ylabel('Velocity [m/s]')\n",
    "        axes[i, j].grid(True)\n",
    "        it += 1  # Increment after plotting each axis\n",
    "fig.suptitle(\"Velocities of the left hand\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        for k in range(5,10):  # Iterate over the first 4 dataframes or less if less than 4\n",
    "            axes[i, j].plot(list_velocities[k][axis[it]])\n",
    "        axes[i, j].set_title(f'Velocity of axis {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Sample')\n",
    "        axes[i, j].set_ylabel('Velocity [m/s]')\n",
    "        axes[i, j].grid(True)\n",
    "        it += 1  # Increment after plotting each axis\n",
    "fig.suptitle(\"Velocities of the right hand\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The axis may be a bit confusing at first. Weren't the accelerations the other way? For our own sanity checks, we have flipped the x-axis to make sure that forward movements are positive. This doesn't affect the features that we will calculate however. What things do you notice in the velocities? Peak velocities really do not appear super different. But let's calculate them anyway. We already have the velocities. So, finding the peak should be relatively trivial. Let's get to it!\n",
    "\n",
    "# Peak Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_peak_vel(velocity, axis= 'all'):\n",
    "    '''\n",
    "    #Input: Velocity and axis\n",
    "    :param velocity: Velocity vector\n",
    "    :param axis: the axis of which to calculate the peak velocity\n",
    "    :return: peak absolute velocity of a given axis.\n",
    "    '''\n",
    "    peak_vel = []\n",
    "    # Loop over the columns to find the peak velocities\n",
    "    for i,col in enumerate(['x', 'y', 'z', 'norm']):\n",
    "        #Calculate the peak velocity and append to peak_vel\n",
    "        peak_vel.append()\n",
    "    return  peak_vel\n",
    "\n",
    "\n",
    "peak_velocities  = []\n",
    "for velocities in list_velocities:\n",
    "        #Append Features to list\n",
    "        peak_velocity = calc_peak_vel(velocities, axis = \"all\")\n",
    "        peak_velocities.append(peak_velocity)\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "trials= [1, 2, 3, 4, 5]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axes[i,j].plot(trials, list(list(zip(*peak_velocities[0:5]))[it]))\n",
    "        axes[i,j].plot(trials, list(list(zip(*peak_velocities[5:]))[it]))\n",
    "        axes[i, j].set_title(f'Peak velocity of axis {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Trial')\n",
    "        axes[i, j].set_xticks(range(1,6))\n",
    "        axes[i, j].set_ylabel('Peak velocity [m/s]')\n",
    "        axes[i, j].grid(True)\n",
    "        axes[i, j].legend([\"Left\", \"Right\"])\n",
    "        it= it + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Interestingly enough, the peak velocities appear highest for the affected arm! There does seem to be quite a variability in the direction of the target. Clearly not enough data to make a strong conclusion about how well the peak velocities can define quality, but good to keep in mind nonetheless. Looking back into the initial velocity figure, that is not really surprising either. A potential explanation might be that the movement itself is not necessarily much slower here, but less efficient.\n",
    "\n",
    "# Movement Smoothness - Log Dimensionless Jerk\n",
    "Let's get into an actual smoothness feature: The log dimensionless jerk. Very briefly, the log dimensionless jerk is the integral of squared absolute jerk (instantaneous rate of change of the accelerations) compensated by a factor (here we use $\\frac{\\text{(movement time)}^{3}}{(\\text{peak velocity)}^2}$). In formula form: $\\text{LDJ} = -ln( \\frac{t_{mov}^{3}}{v_{peak}^{2}} \\cdot \\int_{t0}^{t1}|a(t)dt|^2dt)$. For the integration we recommend using a standard scipy function (like scipy.integrate.simpson). $a(t)$ here is the acceleration. The minus at the end is to make sure that a less negative value also corresponds to a smoother movement.\n",
    "\n",
    "If you aren't an expert in how to convert mathematical functions into code, don't worry. We will talk you through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_log_dimensionless_jerk(peak_vel, acceleration):\n",
    "    '''\n",
    "    #Input: movement_time, peak_vel, acceleration, axis\n",
    "    :param movement_time: Duration of the trial movement\n",
    "    :param peak_vel: The peak velocity during that specific trial\n",
    "    :param acceleration: Acceleration vector\n",
    "    :return: log dimensionless jerk of a given axis\n",
    "    '''\n",
    "    #Step 1 is to calculate the norm of the accelerations. We have been using this in velocities, but haven't made one for the accelerations. If you are lost, look in the code above.\n",
    "    acceleration = acceleration.copy()\n",
    "    acceleration.loc[:, \"norm\"] =\n",
    "\n",
    "    #The jerk is defined as the derivative of the accelerations. An easy way to obtain this in discrete cases is to use the differences between samples. As the time step becomes smaller and smaller, this should converge to the true derivative. np.gradient can do this if we give a vector and the time steps.\n",
    "    abs_jerk_squared =\n",
    "\n",
    "    #We also need the movement time of this trial. Since we shortened the movement, lets re-evaluate it:\n",
    "    movement_time =\n",
    "\n",
    "    #Empty list to store the LDJ for each axis in.\n",
    "    log_dimensionless_jerk = []\n",
    "\n",
    "    # Loop over the columns to compute integrals and log_dimensionless_jerk\n",
    "    for i,col in enumerate(['x', 'y', 'z', 'norm']):\n",
    "        #We need to integrate this over the entire movement, since we have already sliced the data into trials, we can just integrate an entire movement. scipy.integrate has some nice functions for this.\n",
    "        integral_ajs =\n",
    "\n",
    "        #Now we need to make the integral dimensionless by multiplying it by the factor from the text above.\n",
    "        dimensionless_jerk =\n",
    "        #print(integral_ajs)\n",
    "        #And now take the logarithm and multiply it by -1. And append it to the list.\n",
    "        log_dimensionless_jerk.append()\n",
    "    return  log_dimensionless_jerk\n",
    "\n",
    "list_LDJ = []\n",
    "i = 0\n",
    "\n",
    "for acc_side in list_acc_data:\n",
    "    for trial_acc in acc_side:\n",
    "        peak_velocity = peak_velocities[i]\n",
    "        LDJ = calc_log_dimensionless_jerk(peak_velocity, trial_acc)\n",
    "        list_LDJ.append(LDJ)\n",
    "        i = i + 1\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "trials= [1, 2, 3, 4, 5]\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_LDJ[0:5]))[it]))\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_LDJ[5:]))[it]))\n",
    "        axes[i, j].set_title(f'Log dimensionless jerk of {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Trial')\n",
    "        axes[i, j].set_xticks(range(1,6))\n",
    "        axes[i, j].set_ylabel('Log dimensionless jerk [1]')\n",
    "        axes[i, j].grid(True)\n",
    "        axes[i, j].legend([\"Left\", \"Right\"])\n",
    "        it= it + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " In log dimensionless jerk, a less negative means a smoother movement. Here we see that the movements from the left side are indeed smoother than the right - the affected - side. Contrary to the peak velocities, these differences appear consistent in the aprticipant. We should keep in mind that literature says that LDJ is very sensitive to noise and looking at the accelerations. Remember, the jerk is directly related to the changes in the acceleration, if there is a lot of noise in the measurement, the changes due to the movement become difficult to untangle from the noise. This is important to keep in mind in your own applications.\n",
    "\n",
    "# Movement smoothness - SPectral ARClength\n",
    "\n",
    "Perhaps SPARC will do better. SPARC or the SPectral ARClength is calculated by obtaining the fourier transform of the velocity profile within a specific frequency range. Mathematically: $-\\int^{\\omega_{c}}_0\\sqrt{(\\frac{1}{\\omega_c})^2+(\\frac{d\\hat{V}(\\omega)}{d \\omega})^2}d\\omega$ with $\\hat{V}(\\omega) = \\frac{V(\\omega)}{V(0)}$ and $\\omega_c \\triangleq min\\{w_c^{max}, min\\{\\omega, \\hat{V}(r) < \\bar{V} \\text{ } \\forall \\text{ } r>\\omega\\}\\}$. The first term is used to normalize the arc length with respect to the cut-off frequency $\\omega_{c}$\n",
    "\n",
    "Again, don't get scared by the math. This is a more complicated metric to calculate, so we have taken the liberty to do it for you. We followed the MATLAB implementation of Balasubramanian, but with some minor changes to make it work in Python. If you have any questions about the implementation, feel free to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_SPARC(velocity, fs, parameters=[0.05, 20, 4]):\n",
    "    '''\n",
    "    #Input: Velocity, fs and axis optional parameters\n",
    "    :param velocity: Velocity vector\n",
    "    :param fs: Sample frequency\n",
    "    :param axis: Axis of which to calculate the SPARC\n",
    "    :param parameters: Parameters of the SPARC, default [0.05, 20, 4] as recommended by Balasubramanian. The first number is the amplitude threshold. The second number is the maximum frequency cut-off and the third contains the zero padding.\n",
    "    :return: The SPARC of a given velocity profile on a specified axis\n",
    "    '''\n",
    "    #We make an empty list to hold the SPARC for each of the axes\n",
    "    SPARC = []\n",
    "    for i,axis in enumerate(['x', 'y', 'z', 'norm']):\n",
    "        #First we need to determine the number of points for the Fast Fourier Transform\n",
    "        N = len(velocity[axis])\n",
    "        #Add parameter[2] for the zero padding. Lower than 4 is not recommended. Numpy uses the Cooley-Turkey algorithm to calculate the FFT. This algorithm is optimal for data with a length equal to a power of 2. For computational efficiency we can include this, but it is not necessary.\n",
    "        Nfft = int(2**np.ceil(np.log2(N)+parameters[2]))\n",
    "        #Calculate the fast fourier transform of the velocity profile\n",
    "        vel_spect = np.abs(np.fft.fft(velocity[axis], Nfft))\n",
    "\n",
    "        #Create a frequency vector\n",
    "        dt = 1/fs\n",
    "        freq = np.arange(0, ((1/dt)*(Nfft-1)/Nfft) + (1/dt)*(1/Nfft), (1/dt)*(1/Nfft))\n",
    "\n",
    "        #Normalize the spectrum\n",
    "        vel_spect = vel_spect/np.max(vel_spect)\n",
    "\n",
    "        #Find the index that is always above the amplitude threshold and within the cut-off frequency\n",
    "        inxFc = np.where((freq <= parameters[1]) & (vel_spect >= parameters[0]))[0][-1]\n",
    "\n",
    "        #Select the relevant spectrum\n",
    "        vel_spect = vel_spect[0:inxFc]\n",
    "\n",
    "        #Calculate the arc-length\n",
    "        dArcLength = np.sqrt((1/inxFc))**2 + np.diff(vel_spect)**2\n",
    "        #and sum the arc-lengths for the smoothness and multiply by -1\n",
    "        SPARC.append(-1*np.sum(dArcLength))\n",
    "    return SPARC\n",
    "\n",
    "fs = 50\n",
    "i = 0\n",
    "#list_acc_data = [zero_mean_left_acc_hand, zero_mean_right_acc_hand]\n",
    "list_SPARC = []\n",
    "for acc_side in list_acc_data:\n",
    "    for trial_acc in acc_side:\n",
    "        SPARC = calc_SPARC(list_velocities[i], fs)\n",
    "        #Append Features to list\n",
    "        list_SPARC.append(SPARC)\n",
    "        i = i+1\n",
    "\n",
    "num_cols = 2\n",
    "num_rows = 2\n",
    "fig, axes = plt.subplots(num_cols, num_rows, figsize = (16, 8))\n",
    "axis = [\"x\", \"y\", \"z\", \"norm\"]\n",
    "trials= [1, 2, 3, 4, 5]\n",
    "\n",
    "it = 0\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_SPARC[0:5]))[it]))\n",
    "        axes[i,j].plot(trials, list(list(zip(*list_SPARC[5:]))[it]))\n",
    "        axes[i, j].set_title(f'SPARC of {axis[it]}')\n",
    "        axes[i, j].set_xlabel('Trial')\n",
    "        axes[i, j].set_xticks(range(1,6))\n",
    "        axes[i, j].set_ylabel('SPARC [1]')\n",
    "        axes[i, j].grid(True)\n",
    "        axes[i, j].legend([\"Left\", \"Right\"])\n",
    "        it= it + 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here, a less negative SPARC indicate a smoother movement. Here too, we see that the movements of the left hand is smoother than the right hand. Both SPARC and LDJ here are able to indicate a difference in movement smoothness in all axes.\n",
    "\n",
    "# Joint angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_flexion_angle(Trials_R_bf2seg, side):\n",
    "    upper = Trials_R_bf2seg['upperarm_IMU'].tolist()\n",
    "    lower = Trials_R_bf2seg['lowerarm_IMU'].tolist()\n",
    "\n",
    "    elbow_flexion_angle_alltrial = []\n",
    "\n",
    "    for j in range(len(upper)):\n",
    "        elbow_flexion_angle_singletrial = []\n",
    "\n",
    "        for i in range(min(len(upper[j]), len(lower[j]))):\n",
    "            x_wrist_projected = lower[j][i,:,0] - (np.dot(lower[j][i,:,0], lower[j][i,:,2]) / np.linalg.norm(upper[j][i,:,2])**2) * upper[j][i,:,2]\n",
    "            Y = np.linalg.norm(np.cross(x_wrist_projected, upper[j][i,:,1]))\n",
    "            X = np.dot(x_wrist_projected, upper[j][i,:,1])\n",
    "            elbow_flexion_angle = np.arctan2(Y, X)\n",
    "            elbow_flexion_angle = (elbow_flexion_angle / np.pi * 180)*-1+240\n",
    "\n",
    "            if side == \"left\":\n",
    "                elbow_flexion_angle= elbow_flexion_angle - 60\n",
    "\n",
    "            elbow_flexion_angle_singletrial.append(elbow_flexion_angle)\n",
    "\n",
    "        elbow_flexion_angle_alltrial.append(elbow_flexion_angle_singletrial)\n",
    "\n",
    "    return elbow_flexion_angle_alltrial\n",
    "\n",
    "# Calculate elbow flexion angles for left and right sides\n",
    "elbow_flexion_angle_left = elbow_flexion_angle(Trials_R_Left_bf2seg, \"left\")\n",
    "elbow_flexion_angle_right = elbow_flexion_angle(Trials_R_Right_bf2seg, \"right\")\n",
    "\n",
    "\n",
    "# Plot results for left and right sides\n",
    "num_trials = 5\n",
    "fig, axes = plt.subplots(num_trials, 2, figsize=(16, num_trials*4))\n",
    "\n",
    "for i in range(num_trials):\n",
    "    axes[i, 0].plot(elbow_flexion_angle_left[i])\n",
    "    axes[i, 0].set_title('Left Trial {}'.format(i+1))\n",
    "    axes[i, 0].set_xlabel('Time Point')\n",
    "    axes[i, 0].set_ylabel('Elbow Flexion Angle (degrees)')\n",
    "    axes[i, 0].grid(True)\n",
    "\n",
    "    axes[i, 1].plot(elbow_flexion_angle_right[i])\n",
    "    axes[i, 1].set_title('Right Trial {}'.format(i+1))\n",
    "    axes[i, 1].set_xlabel('Time Point')\n",
    "    axes[i, 1].set_ylabel('Elbow Flexion Angle (degrees)')\n",
    "    axes[i, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZic74y864btEBRqSPtj9+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
